# -*- coding: utf-8 -*-
"""TransformnedbasedHAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17EmkR2_oKSoD3gF8gXYd_tkxUAL635kZ
"""

from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
from tqdm import tqdm
from collections import defaultdict

# Initialize Multitask RoBERTa
pretrained_model_name = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)

# Dataset and Tokenization
class MultitaskDataset(Dataset):
    def __init__(self, data, label_columns):
        self.data = data
        self.label_columns = label_columns

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data.iloc[idx]["review"]
        labels = {col: self.data.iloc[idx][col] for col in self.label_columns}
        encoded = tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        return {
            "input_ids": encoded["input_ids"].squeeze(0),
            "attention_mask": encoded["attention_mask"].squeeze(0),
            **labels
        }

# Prepare Data
# data = pd.DataFrame({
#     "review": ["Great product!", "Terrible experience.", "Very satisfied."],
#     "Complain Label": ["non-Complaint", "complaint", "non-Complaint"],
#     "Emotion": ["joy", "anger", "joy"],
#     "Sentiment": ["positive", "negative", "positive"],
#     "Severity": ["no explicit reproach", "accusation", "no explicit reproach"]
# })

label_maps = {
    "Complain Label": {"non-Complaint": 0, "complaint": 1},
    "Emotion": {"anger": 0, "disgust": 1, "fear": 2, "joy": 3, "sadness": 4, "surprise": 5, "other": 6},
    "Sentiment": {"negative": 0, "neutral": 1, "positive": 2},
    "Severity": {"disapproval": 0, "non-complaint": 1, "accusation": 2, "blame": 3, "no explicit reproach": 4}
}

for col in label_maps.keys():
    data[col] = data[col].map(label_maps[col])

train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
train_dataset = MultitaskDataset(train_data, label_maps.keys())
val_dataset = MultitaskDataset(val_data, label_maps.keys())

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# Transformer-based HAN Model with Multitask RoBERTa
class TransformerHAN(nn.Module):
    def __init__(self, pretrained_model_name, output_dims, pretrained_hidden_dim=768, hidden_dim=128):
        super(TransformerHAN, self).__init__()
        self.shared_model = AutoModel.from_pretrained(pretrained_model_name)

        # Projection layer to reduce RoBERTa embeddings
        self.projection_layer = nn.Linear(pretrained_hidden_dim, hidden_dim)

        # Hierarchical attention: sentence and document level
        self.sentence_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=512
            ),
            num_layers=2
        )
        self.document_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=512
            ),
            num_layers=2
        )

        # Task-specific heads
        self.output_layers = nn.ModuleDict({
            key: nn.Linear(hidden_dim, out_dim) for key, out_dim in output_dims.items()
        })

    def forward(self, input_ids, attention_mask):
        # Extract token embeddings from pretrained RoBERTa
        roberta_output = self.shared_model(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = roberta_output.last_hidden_state  # Shape: (batch_size, seq_len, pretrained_hidden_dim)

        # Project token embeddings to hidden_dim
        projected_embeddings = self.projection_layer(token_embeddings)  # Shape: (batch_size, seq_len, hidden_dim)

        # Sentence-level encoding
        sentence_embeddings = self.sentence_encoder(projected_embeddings.permute(1, 0, 2))  # Shape: (seq_len, batch_size, hidden_dim)

        # Document-level encoding
        document_embedding = self.document_encoder(sentence_embeddings)  # Shape: (seq_len, batch_size, hidden_dim)

        # Global pooled representation
        pooled_output = document_embedding.mean(dim=0)  # Shape: (batch_size, hidden_dim)

        # Task-specific logits
        outputs = {key: layer(pooled_output) for key, layer in self.output_layers.items()}
        return outputs

# Initialize Model
output_dims = {key: len(mapping) for key, mapping in label_maps.items()}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TransformerHAN(pretrained_model_name, output_dims).to(device)

# Training and Evaluation Setup
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
criterion = nn.CrossEntropyLoss()

def evaluate_model(model, data_loader, device):
    model.eval()
    all_labels = defaultdict(list)
    all_predictions = defaultdict(list)

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = {key: batch[key].to(device) for key in label_maps.keys()}
            outputs = model(input_ids, attention_mask)

            for key in outputs.keys():
                predictions = torch.argmax(outputs[key], dim=1).cpu().numpy()
                true_labels = labels[key].cpu().numpy()
                all_predictions[key].extend(predictions)
                all_labels[key].extend(true_labels)

    metrics = {}
    for key in label_maps.keys():
        y_true = all_labels[key]
        y_pred = all_predictions[key]
        metrics[key] = {
            "accuracy": accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, average="weighted"),
            "recall": recall_score(y_true, y_pred, average="weighted"),
            "f1_score": f1_score(y_true, y_pred, average="weighted")
        }
    return metrics

def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3):
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} Training"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = {key: batch[key].to(device) for key in label_maps.keys()}
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            losses = {key: criterion(outputs[key], labels[key]) for key in outputs.keys()}
            loss = sum(losses.values())
            train_loss += loss.item()
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1} Training Loss: {train_loss / len(train_loader):.4f}")

        val_metrics = evaluate_model(model, val_loader, device)
        for task, task_metrics in val_metrics.items():
            print(f"Metrics for {task}:")
            print(f"  Accuracy: {task_metrics['accuracy']:.4f}")
            print(f"  Precision: {task_metrics['precision']:.4f}")
            print(f"  Recall: {task_metrics['recall']:.4f}")
            print(f"  F1 Score: {task_metrics['f1_score']:.4f}")

# Train the Model
train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3)

def evaluate_model(model, data_loader, device):
    model.eval()
    all_labels = defaultdict(list)
    all_predictions = defaultdict(list)

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = {key: batch[key].to(device) for key in label_maps.keys()}
            outputs = model(input_ids, attention_mask)

            for key in outputs.keys():
                predictions = torch.argmax(outputs[key], dim=1).cpu().numpy()
                true_labels = labels[key].cpu().numpy()
                all_predictions[key].extend(predictions)
                all_labels[key].extend(true_labels)

    # Calculate metrics
    metrics = {}
    for key in label_maps.keys():
        y_true = all_labels[key]
        y_pred = all_predictions[key]
        metrics[key] = {
            "accuracy": accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, average="weighted"),
            "recall": recall_score(y_true, y_pred, average="weighted"),
            "f1_score": f1_score(y_true, y_pred, average="weighted"),
            # "classification_report": classification_report(y_true, y_pred, target_names=list(label_maps[key].keys()))
        }

    return metrics
# Evaluate the model on the validation set
val_metrics = evaluate_model(model, val_loader, device)

# Print metrics for each task
for task, task_metrics in val_metrics.items():
    print(f"Metrics for {task}:")
    print(f"  Accuracy: {task_metrics['accuracy']:.4f}")
    print(f"  Precision: {task_metrics['precision']:.4f}")
    print(f"  Recall: {task_metrics['recall']:.4f}")
    print(f"  F1 Score: {task_metrics['f1_score']:.4f}")
    # print(f"  Classification Report:\n{task_metrics['classification_report']}")